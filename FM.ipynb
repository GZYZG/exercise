{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc40f8c-72f5-4a2f-b1a3-2cb80c8c3eba",
   "metadata": {},
   "source": [
    "# FM\n",
    "\n",
    "Factorization Machines 实战。\n",
    "\n",
    "先上公式：\n",
    "$$\n",
    "\\hat{y}(\\boldsymbol{x}) = w_0 + \\sum_{i=1}^n w_i \\cdot x_i + \\sum_{i=1}^n \\sum_{j = i+1}^n <\\boldsymbol{v}_i, \\boldsymbol{v}_j>x_i \\cdot x_j \n",
    "$$\n",
    "\n",
    "其中：$\\boldsymbol{v}_i \\in \\mathbb{R}^k, \\boldsymbol{V} \\in \\mathbb{R}^{n \\times k}$，$\\boldsymbol{v}_i$ 是 $\\boldsymbol{V}$ 的第 $i$ 行。数据集 $D = \\{ (\\boldsymbol{x}^{(1)}, y^{(1)}), ... \\}, \\boldsymbol{x}^{(i)} \\in \\mathbb{R}^n$。\n",
    "\n",
    "对 FM 的公式进行化简，主要针对 $\\sum_{i=1}^n \\sum_{j = i+1}^n <\\boldsymbol{v}_i, \\boldsymbol{v}_j>x_i \\cdot x_j$：\n",
    "$$\n",
    "\\begin{aligned}\n",
    " \\sum_{i=1}^n \\sum_{j = i+1}^n <\\boldsymbol{v}_i, \\boldsymbol{v}_j>x_i \\cdot x_j &= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n <\\boldsymbol{v}_i, \\boldsymbol{v}_j>x_i \\cdot x_j - \\frac{1}{2} \\sum_{i=1}^n <\\boldsymbol{v}_i, \\boldsymbol{v}_i>x_i^2 \\\\\n",
    "    &= \\frac{1}{2} ( \\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{if}\\cdot v_{jf} \\cdot x_i \\cdot x_j - \\sum_{i=1}^n \\sum_{f=1}^k v_{if}^2 \\cdot x_i^2) \\\\ \n",
    "    &= \\frac{1}{2} \\sum_{f=1}^k ( \\sum_{i=1}^n \\sum_{j=1}^n v_{if}\\cdot v_{jf} \\cdot x_i \\cdot x_j - \\sum_{i=1}^n v_{if}^2 \\cdot x_i^2)  \\\\\n",
    "    &= \\frac{1}{2} \\sum_{f=1}^k ( (\\sum_{i=1}^n v_{if} \\cdot x_i)  (\\sum_{j=1}^n v_{jf} \\cdot x_j) - \\sum_{i=1}^n v_{if}^2 \\cdot x_i^2)  \\\\\n",
    "    &= \\frac{1}{2} \\sum_{f=1}^k ( (\\sum_{i=1}^n v_{if} \\cdot x_i)^2 - \\sum_{i=1}^n v_{if}^2 \\cdot x_i^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "用 $l(y, \\hat{y})$ 表示损失函数，则对样本 $(\\boldsymbol{x}, y)$ 的损失进行求导：\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\theta} = \\frac{\\partial l}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\theta}\n",
    "$$\n",
    "其中 $\\theta$ 为参数，对不同的参数有：\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}}{\\partial \\theta} = \n",
    "\\begin{cases}\n",
    "1 & \\theta = w_0 \\\\\n",
    "x_i & \\theta = w_i, 1 \\leq i \\leq n \\\\\n",
    "x_i \\sum_{j=1}^n v_{jf}\\cdot x_j - v_{if} x_i^2 & \\theta = v_{if}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "FM 的矩阵形式，令 $\\boldsymbol{X} \\in \\mathbb{R}^{m \\times n}$ 表示 $m$ 个样本，为一批，如何用 FM 对一批数据 $\\boldsymbol{X}$ 进行处理呢？\n",
    "$$\n",
    "\\hat{\\boldsymbol{Y}} = w_0 + \\boldsymbol{X} * \\boldsymbol{w} + \\frac{1}{2} \\{ [(\\boldsymbol{X} * \\boldsymbol{V})^2 - (\\boldsymbol{X} \\cdot \\boldsymbol{X}) * (\\boldsymbol{V} \\cdot \\boldsymbol{V})].sum(axis=1) \\}\n",
    "$$\n",
    "上式中，$*$ 为矩阵乘法，$\\cdot$ 为内积或标量乘法。\n",
    "\n",
    "关于求导的矩阵推导：[FM模型理论之---矩阵形式解读](https://blog.csdn.net/csuyhb/article/details/100575149)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e9294e-6cef-4995-b60e-f8f1dbe8a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import regularizers, Input, layers, optimizers\n",
    "from tensorflow.keras.layers import Layer, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.callbacks import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e4c324-1834-4a90-9043-fcc81fc9c753",
   "metadata": {},
   "source": [
    "# Tensorflow 实现 FM\n",
    "\n",
    "参考：[fun-rec/codes/base_models/FM.py](https://github.com/GZYZG/fun-rec/blob/master/codes/base_models/FM.py)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4e5a2422-f8b1-4d4f-a334-ded66340a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense特征取对数　　sparse特征进行类别编码\n",
    "def process_feat(data, dense_feats, sparse_feats):\n",
    "    df = data.copy()\n",
    "    # dense\n",
    "    df_dense = df[dense_feats].fillna(0.0)\n",
    "    for f in tqdm(dense_feats):\n",
    "        df_dense[f] = df_dense[f].apply(lambda x: np.log(1 + x) if x > -1 else -1)\n",
    "\n",
    "    # sparse\n",
    "    df_sparse = df[sparse_feats].fillna('-1')\n",
    "    for f in tqdm(sparse_feats):\n",
    "        lbe = LabelEncoder()  \n",
    "        df_sparse[f] = lbe.fit_transform(df_sparse[f])  # 将类别特征转化为类别编码\n",
    "\n",
    "    df_sparse_arr = []\n",
    "    for f in tqdm(sparse_feats):\n",
    "        data_new = pd.get_dummies(df_sparse.loc[:, f].values)\n",
    "        data_new.columns = [f + \"_{}\".format(i) for i in range(data_new.shape[1])]\n",
    "        df_sparse_arr.append(data_new)\n",
    "\n",
    "    df_new = pd.concat([df_dense] + df_sparse_arr, axis=1)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "# FM 特征组合层\n",
    "class CrossLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim=10, **kwargs):\n",
    "        super(CrossLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        # 定义交叉特征的权重\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(self.input_dim, self.output_dim),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "    def call(self, x):  # 对照上述公式中的二次项优化公式一起理解\n",
    "        a = K.pow(K.dot(x, self.kernel), 2)\n",
    "        b = K.dot(K.pow(x, 2), K.pow(self.kernel, 2))\n",
    "        return 0.5 * K.mean(a - b, 1, keepdims=True)\n",
    "\n",
    "# 继承 Model 定义 FM\n",
    "class _FM(Model):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(FM, self).__init__(name=\"FM\")\n",
    "        self.inputs = Input(feature_dim)\n",
    "        self.linear = Dense(units=1,\n",
    "                            kernel_regularizer=regularizers.l2(0.01), \n",
    "                            bias_regularizer=regularizers.l1(0.01))\n",
    "        self.cross = CrossLayer(feature_dim)\n",
    "        self.add = Add()\n",
    "        self.pred = Dense(units=1, activation=\"sigmoid\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        i = inputs # self.inputs(inputs)\n",
    "        linear = self.linear(i)\n",
    "        cross = self.cross(inputs)\n",
    "        add = self.add([linear, cross])\n",
    "        pred = self.pred(add)\n",
    "        return pred\n",
    "    \n",
    "\n",
    "# 函数式定义FM模型\n",
    "def FM(feature_dim):\n",
    "    inputs = Input(shape=(feature_dim,))\n",
    "\n",
    "    # 一阶特征\n",
    "    linear = Dense(units=1,  # units 指输出的维数，不用指定输入的维数\n",
    "                   kernel_regularizer=regularizers.l2(0.01),  # 针对权重的正则化\n",
    "                   bias_regularizer=regularizers.l2(0.01))(inputs)  # 针对偏置的正则化\n",
    "\n",
    "    # 二阶特征\n",
    "    cross = CrossLayer(feature_dim)(inputs)\n",
    "    add = Add()([linear, cross])  # 将一阶特征与二阶特征相加构建FM模型\n",
    "\n",
    "    pred = Dense(units=1, activation=\"sigmoid\")(add)\n",
    "    model = Model(inputs=inputs, outputs=pred, name=\"FM_function\")\n",
    "\n",
    "    model.summary()\n",
    "#     model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer=optimizers.Adam(),\n",
    "#                   metrics=['binary_accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56ef174c-a2fd-41b1-a689-2756d4c490b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "processing features\n",
      "unprocessed feats shape: (1599, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 351.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 1079.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 296.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed feats shape: (1599, 12605)\n",
      "Model: \"FM_function\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 12605)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1)            12606       input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cross_layer_18 (CrossLayer)     (None, 1)            126050      input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 1)            0           dense_37[0][0]                   \n",
      "                                                                 cross_layer_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 1)            2           add_18[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 138,658\n",
      "Trainable params: 138,658\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 1s 58ms/step - loss: 0.6080 - binary_accuracy: 0.7780 - val_loss: 0.5529 - val_binary_accuracy: 0.8094\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5281 - binary_accuracy: 0.7866 - val_loss: 0.5064 - val_binary_accuracy: 0.8094\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4888 - binary_accuracy: 0.7866 - val_loss: 0.4898 - val_binary_accuracy: 0.8094\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4658 - binary_accuracy: 0.7866 - val_loss: 0.4858 - val_binary_accuracy: 0.8094\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.4474 - binary_accuracy: 0.7866 - val_loss: 0.4821 - val_binary_accuracy: 0.8094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18292d1e370>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "print('loading data...')\n",
    "data = pd.read_csv('./data/kaggle_train.csv')\n",
    "\n",
    "# dense 特征开头是I，sparse特征开头是C，Label是标签\n",
    "cols = data.columns.values[1:]\n",
    "dense_feats = [f for f in cols if f[0] == 'I']\n",
    "sparse_feats = [f for f in cols if f[0] == 'C']\n",
    "sparse_feats.insert(0, 'Id')\n",
    "\n",
    "# 对dense数据和sparse数据分别处理\n",
    "print('processing features')\n",
    "print(f\"unprocessed feats shape: {data.shape}\")\n",
    "feats = process_feat(data, dense_feats, sparse_feats)\n",
    "print(f\"processed feats shape: {feats.shape}\")\n",
    "\n",
    "# 划分训练和验证数据\n",
    "x_trn, x_tst, y_trn, y_tst = train_test_split(feats, data['Label'], test_size=0.2, random_state=2022)\n",
    "\n",
    "# 定义模型\n",
    "model = FM(feats.shape[1])\n",
    "\n",
    "# model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(x_trn, y_trn, epochs=5, batch_size=256, validation_data=(x_tst, y_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32159c-5664-454d-b932-d020f70424ba",
   "metadata": {},
   "source": [
    "# Numpy 实现\n",
    "\n",
    "不使用现有框架求导的话，则需要自己实现求导的过程。求导过程主要涉及到：$\\frac{\\partial l(y, \\hat{y})}{\\partial \\hat{y}}$ 和 $\\frac{\\partial \\hat{y}}{\\partial \\theta}$。其中 $\\frac{\\partial \\hat{y}}{\\partial \\theta}$ 是固定的，但是对于不同的损失函数 $l$，$\\frac{\\partial l(y, \\hat{y})}{\\partial \\hat{y}}$ 则需要给出对应的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e564fd7f-32e3-4eae-81aa-abc0dfbbbddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.74725189, -1.35465212]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(-4*np.sqrt(6 / (1+2)), 4*np.sqrt(6 / (1+2)), size=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bcb08ef0-0236-43af-a45d-35e075502f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def cross_entropy(ytrue, pred, reduce=False):\n",
    "    \"\"\"\n",
    "    交叉熵损失函数\n",
    "    \"\"\"\n",
    "    n = len(ytrue)\n",
    "#     print(max(pred), min(pred))\n",
    "    l = ytrue * np.log(pred) + (1 - ytrue) * np.log(1e-7+1 - pred)\n",
    "    if reduce:\n",
    "        l = l.sum() / n\n",
    "    \n",
    "    # 求 l 对 pred 的导数\n",
    "    grad = pred - ytrue\n",
    "    \n",
    "    return -l, grad\n",
    "    \n",
    "    \n",
    "class FM:\n",
    "    def __init__(self, feature_dim, factor_dim=10, learning_rate=0.01, loss=\"cross_entropy\"):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.factor_dim = factor_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        \n",
    "        self.init()\n",
    "        \n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        创建参数\n",
    "        -----------\n",
    "        注意参数初始化的方式：\n",
    "        - 从随机正态分布中采样\n",
    "        \"\"\"\n",
    "        self.bias = np.random.uniform(-4*np.sqrt(3), 4*np.sqrt(3)) \n",
    "        self.linear_weight = np.random.uniform(-4*np.sqrt(6 / (self.feature_dim + 1)), 4*np.sqrt(6 / (self.feature_dim + 1)), size=(self.feature_dim, ))\n",
    "        self.cross_weight = np.random.uniform(-4*np.sqrt(6 / (self.feature_dim+self.factor_dim)), \n",
    "                                              4*np.sqrt(6 / (self.feature_dim+self.factor_dim)), \n",
    "                                              size=(self.feature_dim, self.factor_dim))\n",
    "    \n",
    "    def _forward_batch(self, X):\n",
    "        m = len(X)\n",
    "        X = X.reshape((m, self.feature_dim))\n",
    "        y_hat = self.bias + X @ self.linear_weight.reshape((-1,1)) + 0.5 * ( np.power(X@self.cross_weight, 2) - (X**2)@(self.cross_weight**2)).sum(axis=1, keepdims=1)\n",
    "#         print(( np.power(X@self.cross_weight, 2) - (X**2)@(self.cross_weight**2)).sum(axis=1, keepdims=True).shape)\n",
    "        return y_hat.flatten()\n",
    "    \n",
    "    def forward(self, X, use_batch=True):\n",
    "        \"\"\"\n",
    "        完成一次前向，返回各个样本的预测值. 可以逐个样本计算，也可以一次性算完所有的。\n",
    "        -----------\n",
    "        X: (n_sample, n_feature)\n",
    "        \"\"\"\n",
    "        if use_batch:\n",
    "            return self._forward_batch(X)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        y_hat = np.empty(m)\n",
    "        \n",
    "        for i in range(m):\n",
    "            x = X[i, :]\n",
    "            tmp = self.bias\n",
    "            tmp += (x * self.linear_weight).sum()\n",
    "            for j in range(self.factor_dim):\n",
    "                tmp += (np.power((self.cross_weight[:, j] * x).sum(), 2) - np.power(self.cross_weight[:, j] * x, 2).sum()) / 2\n",
    "            y_hat[i] = tmp\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        使用 X, y 完成一次参数更新，mini-BGD\n",
    "        -----------\n",
    "        X: (n_sample, n_feature)\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        y_hat = self.forward(X)\n",
    "        pred = sigmoid(y_hat)\n",
    "#         print(pred)\n",
    "        loss, grads = cross_entropy(y, pred)\n",
    "        \n",
    "        # 小批量梯度下降更新参数\n",
    "        self.bias = self.bias - self.learning_rate * grads.mean()\n",
    "        self.linear_weight = self.linear_weight - self.learning_rate * (grads.reshape((-1,1)) * X).mean(axis=0)\n",
    "#         self.cross_weight = self.cross_weight - self.learning_rate * grads.reshape((-1, 1)) * ( X.T @ (X @ self.cross_weight) - (X**2).T @ self.cross_weight )\n",
    "        \n",
    "        for i in range(self.factor_dim):\n",
    "            v = self.cross_weight[:, i]  # (n_features, )\n",
    "            tmp = (v * X).sum(axis=1)  # (n_sample, )\n",
    "            tmp = tmp.reshape((-1, 1)) * X  # (n_sample, n_feature)\n",
    "            tmp = grads.reshape((-1,1)) * tmp\n",
    "            tmp = tmp.mean(axis=0)  # (n_features, )\n",
    "            \n",
    "            tmp2 = X ** 2  # (n_sample, n_feature)\n",
    "            tmp2 = tmp2 * v  # (n_sample, n_feature)\n",
    "            tmp2 = grads.reshape((-1,1)) * tmp2\n",
    "            tmp2 = tmp2.mean(axis=0)\n",
    "            \n",
    "            self.cross_weight[:, i] = self.cross_weight[:, i] - self.learning_rate * (tmp - tmp2)\n",
    "      \n",
    "        acc = (y == (pred > 0.5)).sum() / m\n",
    "        \n",
    "#         print(f\"loss: {loss.mean()}\\tacc: {acc}\")\n",
    "        return loss.mean(), acc\n",
    "    \n",
    "    def predict(self, X, use_batch=True, logits=True):\n",
    "        X = np.array(X)\n",
    "        y_hat = self.forward(X, use_batch=use_batch)\n",
    "        \n",
    "        if logits:\n",
    "            return sigmoid(y_hat)\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f9589a14-bf63-4fdf-bb3b-118edffb737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "processing features\n",
      "unprocessed feats shape: (1599, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 220.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 415.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 264.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed feats shape: (1599, 12605)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "print('loading data...')\n",
    "data = pd.read_csv('./data/kaggle_train.csv')\n",
    "\n",
    "# dense 特征开头是I，sparse特征开头是C，Label是标签\n",
    "cols = data.columns.values[1:]\n",
    "dense_feats = [f for f in cols if f[0] == 'I']\n",
    "sparse_feats = [f for f in cols if f[0] == 'C']\n",
    "sparse_feats.insert(0, 'Id')\n",
    "\n",
    "# 对dense数据和sparse数据分别处理\n",
    "print('processing features')\n",
    "print(f\"unprocessed feats shape: {data.shape}\")\n",
    "feats = process_feat(data, dense_feats, sparse_feats)\n",
    "print(f\"processed feats shape: {feats.shape}\")\n",
    "\n",
    "feats = np.array(feats)\n",
    "labels = np.array(data['Label'])\n",
    "\n",
    "# 划分训练和验证数据\n",
    "x_trn, x_tst, y_trn, y_tst = train_test_split(feats, labels, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8390bd79-1015-429c-9f0c-8982ebc3d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FM(feats.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b3b48298-f86d-4d65-88c0-89f31fc3995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Epoch: 0 ******************************\n",
      "Step   0: train loss: 0.46808 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.57565 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.50376 train acc:0.80469 =========================\n",
      "Step   3: train loss: 0.45999 train acc:0.78125 =========================\n",
      "Step   4: train loss: 0.45842 train acc:0.78906 =========================\n",
      "Step   5: train loss: 0.57725 train acc:0.79688 =========================\n",
      "Step   6: train loss: 0.46069 train acc:0.77344 =========================\n",
      "Step   7: train loss: 0.40897 train acc:0.85938 =========================\n",
      "Step   8: train loss: 0.51686 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.52043 train acc:0.77165 =========================\n",
      "Epoch  0: acc: 0.8125\n",
      "****************************** Epoch: 1 ******************************\n",
      "Step   0: train loss: 0.46385 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.57101 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.50043 train acc:0.80469 =========================\n",
      "Step   3: train loss: 0.45890 train acc:0.77344 =========================\n",
      "Step   4: train loss: 0.45426 train acc:0.78906 =========================\n",
      "Step   5: train loss: 0.57220 train acc:0.79688 =========================\n",
      "Step   6: train loss: 0.45875 train acc:0.78125 =========================\n",
      "Step   7: train loss: 0.40572 train acc:0.85938 =========================\n",
      "Step   8: train loss: 0.51316 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.51516 train acc:0.77953 =========================\n",
      "Epoch  1: acc: 0.809375\n",
      "****************************** Epoch: 2 ******************************\n",
      "Step   0: train loss: 0.45984 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.56657 train acc:0.75000 =========================\n",
      "Step   2: train loss: 0.49712 train acc:0.81250 =========================\n",
      "Step   3: train loss: 0.45782 train acc:0.77344 =========================\n",
      "Step   4: train loss: 0.45032 train acc:0.79688 =========================\n",
      "Step   5: train loss: 0.56741 train acc:0.80469 =========================\n",
      "Step   6: train loss: 0.45691 train acc:0.78906 =========================\n",
      "Step   7: train loss: 0.40255 train acc:0.85938 =========================\n",
      "Step   8: train loss: 0.50959 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.51008 train acc:0.77953 =========================\n",
      "Epoch  2: acc: 0.809375\n",
      "****************************** Epoch: 3 ******************************\n",
      "Step   0: train loss: 0.45605 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.56232 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.49381 train acc:0.81250 =========================\n",
      "Step   3: train loss: 0.45673 train acc:0.77344 =========================\n",
      "Step   4: train loss: 0.44657 train acc:0.79688 =========================\n",
      "Step   5: train loss: 0.56286 train acc:0.80469 =========================\n",
      "Step   6: train loss: 0.45516 train acc:0.78906 =========================\n",
      "Step   7: train loss: 0.39946 train acc:0.85938 =========================\n",
      "Step   8: train loss: 0.50616 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.50518 train acc:0.77953 =========================\n",
      "Epoch  3: acc: 0.80625\n",
      "****************************** Epoch: 4 ******************************\n",
      "Step   0: train loss: 0.45245 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.55824 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.49052 train acc:0.81250 =========================\n",
      "Step   3: train loss: 0.45564 train acc:0.77344 =========================\n",
      "Step   4: train loss: 0.44300 train acc:0.79688 =========================\n",
      "Step   5: train loss: 0.55854 train acc:0.80469 =========================\n",
      "Step   6: train loss: 0.45349 train acc:0.78125 =========================\n",
      "Step   7: train loss: 0.39643 train acc:0.85938 =========================\n",
      "Step   8: train loss: 0.50286 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.50044 train acc:0.77953 =========================\n",
      "Epoch  4: acc: 0.80625\n",
      "****************************** Epoch: 5 ******************************\n",
      "Step   0: train loss: 0.44902 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.55431 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.48724 train acc:0.81250 =========================\n",
      "Step   3: train loss: 0.45453 train acc:0.78125 =========================\n",
      "Step   4: train loss: 0.43958 train acc:0.80469 =========================\n",
      "Step   5: train loss: 0.55442 train acc:0.81250 =========================\n",
      "Step   6: train loss: 0.45187 train acc:0.78125 =========================\n",
      "Step   7: train loss: 0.39346 train acc:0.86719 =========================\n",
      "Step   8: train loss: 0.49968 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.49584 train acc:0.77953 =========================\n",
      "Epoch  5: acc: 0.80625\n",
      "****************************** Epoch: 6 ******************************\n",
      "Step   0: train loss: 0.44574 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.55053 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.48396 train acc:0.81250 =========================\n",
      "Step   3: train loss: 0.45339 train acc:0.78125 =========================\n",
      "Step   4: train loss: 0.43632 train acc:0.80469 =========================\n",
      "Step   5: train loss: 0.55050 train acc:0.81250 =========================\n",
      "Step   6: train loss: 0.45029 train acc:0.78906 =========================\n",
      "Step   7: train loss: 0.39054 train acc:0.85938 =========================\n",
      "Step   8: train loss: 0.49662 train acc:0.76562 =========================\n",
      "Step   9: train loss: 0.49138 train acc:0.77953 =========================\n",
      "Epoch  6: acc: 0.80625\n",
      "****************************** Epoch: 7 ******************************\n",
      "Step   0: train loss: 0.44260 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.54689 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.48070 train acc:0.81250 =========================\n",
      "Step   3: train loss: 0.45223 train acc:0.78125 =========================\n",
      "Step   4: train loss: 0.43318 train acc:0.80469 =========================\n",
      "Step   5: train loss: 0.54675 train acc:0.81250 =========================\n",
      "Step   6: train loss: 0.44873 train acc:0.79688 =========================\n",
      "Step   7: train loss: 0.38768 train acc:0.85938 =========================\n",
      "Step   8: train loss: 0.49367 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.48703 train acc:0.77953 =========================\n",
      "Epoch  7: acc: 0.80625\n",
      "****************************** Epoch: 8 ******************************\n",
      "Step   0: train loss: 0.43959 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.54336 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.47744 train acc:0.81250 =========================\n",
      "Step   3: train loss: 0.45104 train acc:0.78125 =========================\n",
      "Step   4: train loss: 0.43016 train acc:0.80469 =========================\n",
      "Step   5: train loss: 0.54317 train acc:0.81250 =========================\n",
      "Step   6: train loss: 0.44720 train acc:0.80469 =========================\n",
      "Step   7: train loss: 0.38487 train acc:0.84375 =========================\n",
      "Step   8: train loss: 0.49082 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.48279 train acc:0.77953 =========================\n",
      "Epoch  8: acc: 0.809375\n",
      "****************************** Epoch: 9 ******************************\n",
      "Step   0: train loss: 0.43670 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.53993 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.47420 train acc:0.82031 =========================\n",
      "Step   3: train loss: 0.44981 train acc:0.78906 =========================\n",
      "Step   4: train loss: 0.42724 train acc:0.80469 =========================\n",
      "Step   5: train loss: 0.53974 train acc:0.81250 =========================\n",
      "Step   6: train loss: 0.44566 train acc:0.79688 =========================\n",
      "Step   7: train loss: 0.38210 train acc:0.84375 =========================\n",
      "Step   8: train loss: 0.48807 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.47865 train acc:0.78740 =========================\n",
      "Epoch  9: acc: 0.809375\n",
      "****************************** Epoch: 10 ******************************\n",
      "Step   0: train loss: 0.43391 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.53661 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.47096 train acc:0.82031 =========================\n",
      "Step   3: train loss: 0.44853 train acc:0.78906 =========================\n",
      "Step   4: train loss: 0.42442 train acc:0.81250 =========================\n",
      "Step   5: train loss: 0.53644 train acc:0.80469 =========================\n",
      "Step   6: train loss: 0.44412 train acc:0.79688 =========================\n",
      "Step   7: train loss: 0.37938 train acc:0.84375 =========================\n",
      "Step   8: train loss: 0.48541 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.47460 train acc:0.78740 =========================\n",
      "Epoch 10: acc: 0.8125\n",
      "****************************** Epoch: 11 ******************************\n",
      "Step   0: train loss: 0.43121 train acc:0.81250 =========================\n",
      "Step   1: train loss: 0.53337 train acc:0.76562 =========================\n",
      "Step   2: train loss: 0.46774 train acc:0.82031 =========================\n",
      "Step   3: train loss: 0.44722 train acc:0.78906 =========================\n",
      "Step   4: train loss: 0.42168 train acc:0.82031 =========================\n",
      "Step   5: train loss: 0.53328 train acc:0.80469 =========================\n",
      "Step   6: train loss: 0.44257 train acc:0.79688 =========================\n",
      "Step   7: train loss: 0.37669 train acc:0.84375 =========================\n",
      "Step   8: train loss: 0.48283 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.47062 train acc:0.78740 =========================\n",
      "Epoch 11: acc: 0.809375\n",
      "****************************** Epoch: 12 ******************************\n",
      "Step   0: train loss: 0.42860 train acc:0.82031 =========================\n",
      "Step   1: train loss: 0.53022 train acc:0.76562 =========================\n",
      "Step   2: train loss: 0.46454 train acc:0.82031 =========================\n",
      "Step   3: train loss: 0.44587 train acc:0.78906 =========================\n",
      "Step   4: train loss: 0.41901 train acc:0.82031 =========================\n",
      "Step   5: train loss: 0.53022 train acc:0.81250 =========================\n",
      "Step   6: train loss: 0.44101 train acc:0.79688 =========================\n",
      "Step   7: train loss: 0.37403 train acc:0.84375 =========================\n",
      "Step   8: train loss: 0.48032 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.46673 train acc:0.78740 =========================\n",
      "Epoch 12: acc: 0.80625\n",
      "****************************** Epoch: 13 ******************************\n",
      "Step   0: train loss: 0.42606 train acc:0.82031 =========================\n",
      "Step   1: train loss: 0.52713 train acc:0.76562 =========================\n",
      "Step   2: train loss: 0.46134 train acc:0.82031 =========================\n",
      "Step   3: train loss: 0.44447 train acc:0.78906 =========================\n",
      "Step   4: train loss: 0.41641 train acc:0.82031 =========================\n",
      "Step   5: train loss: 0.52727 train acc:0.80469 =========================\n",
      "Step   6: train loss: 0.43943 train acc:0.79688 =========================\n",
      "Step   7: train loss: 0.37142 train acc:0.84375 =========================\n",
      "Step   8: train loss: 0.47788 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.46289 train acc:0.78740 =========================\n",
      "Epoch 13: acc: 0.803125\n",
      "****************************** Epoch: 14 ******************************\n",
      "Step   0: train loss: 0.42360 train acc:0.82031 =========================\n",
      "Step   1: train loss: 0.52410 train acc:0.75781 =========================\n",
      "Step   2: train loss: 0.45816 train acc:0.82031 =========================\n",
      "Step   3: train loss: 0.44303 train acc:0.78906 =========================\n",
      "Step   4: train loss: 0.41387 train acc:0.82031 =========================\n",
      "Step   5: train loss: 0.52441 train acc:0.80469 =========================\n",
      "Step   6: train loss: 0.43782 train acc:0.80469 =========================\n",
      "Step   7: train loss: 0.36883 train acc:0.85156 =========================\n",
      "Step   8: train loss: 0.47550 train acc:0.77344 =========================\n",
      "Step   9: train loss: 0.45912 train acc:0.78740 =========================\n",
      "Epoch 14: acc: 0.8\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "batch_size =128\n",
    "x_trn \n",
    "m = len(x_trn)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    nb = int(np.ceil(m / batch_size))\n",
    "    print(f\"{'*'*30} Epoch: {epoch} {'*'*30}\")\n",
    "    for b in range(nb):\n",
    "        x_batch = x_trn[b*batch_size: (b+1)*batch_size]\n",
    "        y_batch = y_trn[b*batch_size: (b+1)*batch_size]\n",
    "        loss, acc = model.fit(x_batch, y_batch)\n",
    "        print(f\"Step{b:>4}: train loss: {loss:^6.5f} train acc:{acc:^6.5f} {'='*25}\")\n",
    "    pred = model.predict(x_tst)\n",
    "    pred = (pred > 0.5).astype(int)\n",
    "    acc = (pred == y_tst).astype(int).mean()\n",
    "    print(f\"Epoch{epoch:>3}: acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "61463603-0340-41ec-868b-3ce0c3f32095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_tst)\n",
    "pred = (pred > 0.5).astype(int)\n",
    "acc = (pred == y_tst).astype(int).mean()\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac36578-a1fd-446d-bd7b-58350f2dceae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
